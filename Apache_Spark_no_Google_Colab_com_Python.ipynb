{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONA230FRA7xT0hw5nufaWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdvaldoFilhoPro/Apache_Spark_no_Google_Colab_com_Python/blob/main/Apache_Spark_no_Google_Colab_com_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Essas etapas configuram o ambiente para utilizar o Apache Spark no Google Colab. O Spark é um framework poderoso para processamento de big data e computação distribuída, e com essa configuração, você pode começar a utilizar suas capacidades para processar e analisar grandes volumes de dados de forma eficiente.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4TidjcxHLFlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação do Java: É realizada a instalação do OpenJDK 8 (Java) no ambiente para que o Spark funcione corretamente."
      ],
      "metadata": {
        "id": "s5piOLJ6EM2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação do Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "7SknGMbaEPgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download do Spark: O arquivo binário do Spark é baixado a partir de um link específico. O Spark é um framework de processamento de dados distribuído e computação em cluster."
      ],
      "metadata": {
        "id": "dEZN2Zf5Eg8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo o download do Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "LLQgGPU3EqbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descompactação dos arquivos: O arquivo compactado do Spark é descompactado para a pasta especificada."
      ],
      "metadata": {
        "id": "hmZJVWOGE-yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descompactando os arquivos do Spark\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "6KmKY6-bFHto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação da biblioteca pyspark: É realizada a instalação da biblioteca pyspark que é uma API Python para o Spark."
      ],
      "metadata": {
        "id": "sYQTRIx3FXcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando a biblioteca PySpark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8ZaXGVnFZrS",
        "outputId": "b117b92c-1e93-4fa1-e631-f3257e2a3011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação da biblioteca findspark: É realizada a instalação da biblioteca findspark que permite encontrar automaticamente a instalação do Spark."
      ],
      "metadata": {
        "id": "eQE7-I17Fkpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando a biblioteca findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "tuj5XSLoFzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da biblioteca os: A biblioteca os é importada para permitir a configuração das variáveis de ambiente."
      ],
      "metadata": {
        "id": "UezTQ3s1F4UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a biblioteca os\n",
        "import os"
      ],
      "metadata": {
        "id": "J-oUFhciGBI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração da variável de ambiente JAVA_HOME: É definida a variável de ambiente JAVA_HOME com o caminho para a instalação do Java."
      ],
      "metadata": {
        "id": "m-bGCkAiGFns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a variável de ambiente do Java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "BUpAVvgiGFJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuração da variável de ambiente `SPARK_HOME`: É definida a variável de ambiente `SPARK_HOME` com o caminho para a instalação do Spark."
      ],
      "metadata": {
        "id": "pUlK1ogcGTPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a variável de ambiente do Spark\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "6TYyet5WGXaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da biblioteca `findspark`: A biblioteca `findspark` é importada para ser utilizada em seguida."
      ],
      "metadata": {
        "id": "qCZ9xx2dGrpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando a biblioteca findspark\n",
        "import findspark"
      ],
      "metadata": {
        "id": "I1OljU57HbCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialização do `findspark`: A função `findspark.init()` é chamada para permitir encontrar o Spark."
      ],
      "metadata": {
        "id": "e5m4La1mJXy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciando o findspark para encontrar o Spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "x7fA34hPJpy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação do pacote necessário para iniciar uma sessão Spark: O pacote `SparkSession` é importado para permitir criar uma sessão Spark."
      ],
      "metadata": {
        "id": "hVluqdgQJxWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o pacote necessário para iniciar uma sessão Spark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "lsnghZkIJ_x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialização do SparkContext e criação da sessão Spark: Através da chamada `SparkSession.builder.master('local[*]').getOrCreate()`, é criada uma sessão Spark que se comunica com o cluster local."
      ],
      "metadata": {
        "id": "60lRkGBSKF3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "HmjbeD7JKWcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se a sessão Spark foi criada com sucesso\n",
        "s1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tUm355HD62B",
        "outputId": "61fbeda9-fce6-4a9b-a560-33c2211024cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78546091f4f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9225e85cc929:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}